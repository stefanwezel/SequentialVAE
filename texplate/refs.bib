@article{Greenberg2019,
abstract = {How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.},
archivePrefix = {arXiv},
arxivId = {1905.07488},
author = {Greenberg, David S. and Nonnenmacher, Marcel and Macke, Jakob H.},
eprint = {1905.07488},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Greenberg, Nonnenmacher, Macke{\_}2019{\_}Automatic posterior transformation for likelihood-free inference.pdf:pdf},
isbn = {9781510886988},
journal = {36th International Conference on Machine Learning, ICML 2019},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
pages = {4288--4304},
title = {{Automatic posterior transformation for likelihood-free inference}},
volume = {2019-June},
year = {2019}
}
@article{Goncalves2020,
abstract = {Mechanistic modeling in neuroscience aims to explain observed phenomena in terms of underlying causes. However, determining which model parameters agree with complex and stochastic neural data presents a significant challenge. We address this challenge with a machine learning tool which uses deep neural density estimators— trained using model simulations— to carry out Bayesian inference and retrieve the full space of parameters compatible with raw data or selected data features. Our method is scalable in parameters and data features, and can rapidly analyze new data after initial training. We demonstrate the power and fiexibility of our approach on receptive fields, ion channels, and Hodgkin–Huxley models. We also characterize the space of circuit configurations giving rise to rhythmic activity in the crustacean stomatogastric ganglion, and use these results to derive hypotheses for underlying compensation mechanisms. Our approach will help close the gap between data-driven and theory-driven models of neural dynamics.},
author = {Gon{\c{c}}alves, Pedro J. and Lueckmann, Jan Matthis and Deistler, Michael and Nonnenmacher, Marcel and {\"{O}}cal, Kaan and Bassetto, Giacomo and Chintaluri, Chaitanya and Podlaski, William F. and Haddad, Sara A. and Vogels, Tim P. and Greenberg, David S. and Macke, Jakob H.},
doi = {10.7554/ELIFE.56261},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Gon{\c{c}}alves et al.{\_}2020{\_}Training deep neural density estimators to identify mechanistic models of neural dynamics.pdf:pdf},
issn = {2050084X},
journal = {eLife},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
pages = {1--46},
pmid = {32940606},
title = {{Training deep neural density estimators to identify mechanistic models of neural dynamics}},
volume = {9},
year = {2020}
}
@article{Ardizzone2019,
abstract = {For many applications, in particular in natural science, the task is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is well-defined, whereas the inverse problem is ambiguous: multiple parameter sets can result in the same measurement. To fully characterize this ambiguity, the full posterior parameter distribution, conditioned on an observed measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task - so-called Invertible Neural Networks (INNs). Unlike classical neural networks, which attempt to solve the ambiguous inverse problem directly, INNs focus on learning the forward process, using additional latent output variables to capture the information otherwise lost. Due to invertibility, a model of the corresponding inverse process is learned implicitly. Given a specific measurement and the distribution of the latent variables, the inverse pass of the INN provides the full posterior over parameter space. We prove theoretically and verify experimentally, on artificial data and real-world problems from medicine and astrophysics, that INNs are a powerful analysis tool to find multi-modalities in parameter space, uncover parameter correlations, and identify unrecoverable parameters.},
archivePrefix = {arXiv},
arxivId = {1808.04730},
author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and K{\"{o}}the, Ullrich},
eprint = {1808.04730},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Ardizzone et al.{\_}2019{\_}Analyzing inverse problems with invertible neural networks.pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
number = {i},
pages = {1--20},
title = {{Analyzing inverse problems with invertible neural networks}},
year = {2019}
}
@article{Brehmer2020,
abstract = {Our predictions for particle physics processes are realized in a chain of complex simulators. They allow us to generate high-fidelty simulated data, but they are not well-suited for inference on the theory parameters with observed data. We explain why the likelihood function of high-dimensional LHC data cannot be explicitly evaluated, why this matters for data analysis, and reframe what the field has traditionally done to circumvent this problem. We then review new simulation-based inference methods that let us directly analyze high-dimensional data by combining machine learning techniques and information from the simulator. Initial studies indicate that these techniques have the potential to substantially improve the precision of LHC measurements. Finally, we discuss probabilistic programming, an emerging paradigm that lets us extend inference to the latent process of the simulator.},
archivePrefix = {arXiv},
arxivId = {2010.06439},
author = {Brehmer, Johann and Cranmer, Kyle},
eprint = {2010.06439},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Brehmer, Cranmer{\_}2020{\_}Simulation-based inference methods for particle physics.pdf:pdf},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/machine learning for scientific discovery,01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
title = {{Simulation-based inference methods for particle physics}},
url = {http://arxiv.org/abs/2010.06439},
year = {2020}
}
@article{Green2020,
abstract = {We introduce the use of autoregressive normalizing flows for rapid likelihood-free inference of binary black hole system parameters from gravitational-wave data with deep neural networks. A normalizing flow is an invertible mapping on a sample space that can be used to induce a transformation from a simple probability distribution to a more complex one: if the simple distribution can be rapidly sampled and its density evaluated, then so can the complex distribution. Our first application to gravitational waves uses an autoregressive flow, conditioned on detector strain data, to map a multivariate standard normal distribution into the posterior distribution over system parameters. We train the model on artificial strain data consisting of IMRPhenomPv2 waveforms drawn from a five-parameter {\$}(m{\_}1, m{\_}2, \backslashphi{\_}0, t{\_}c, d{\_}L){\$} prior and stationary Gaussian noise realizations with a fixed power spectral density. This gives performance comparable to current best deep-learning approaches to gravitational-wave parameter estimation. We then build a more powerful latent variable model by incorporating autoregressive flows within the variational autoencoder framework. This model has performance comparable to Markov chain Monte Carlo and, in particular, successfully models the multimodal {\$}\backslashphi{\_}0{\$} posterior. Finally, we train the autoregressive latent variable model on an expanded parameter space, including also aligned spins {\$}(\backslashchi{\_}{\{}1z{\}}, \backslashchi{\_}{\{}2z{\}}){\$} and binary inclination {\$}\backslashtheta{\_}{\{}JN{\}}{\$}, and show that all parameters and degeneracies are well-recovered. In all cases, sampling is extremely fast, requiring less than two seconds to draw {\$}10{\^{}}4{\$} posterior samples.},
archivePrefix = {arXiv},
arxivId = {2002.07656},
author = {Green, Stephen R. and Simpson, Christine and Gair, Jonathan},
eprint = {2002.07656},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Green, Simpson, Gair{\_}2020{\_}Gravitational-wave parameter estimation with autoregressive neural network flows.pdf:pdf},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
pages = {1--14},
title = {{Gravitational-wave parameter estimation with autoregressive neural network flows}},
url = {http://arxiv.org/abs/2002.07656},
year = {2020}
}
@article{Papamakarios2020,
abstract = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.},
archivePrefix = {arXiv},
arxivId = {1805.07226},
author = {Papamakarios, George and Sterratt, David C. and Murray, Iain},
eprint = {1805.07226},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Papamakarios, Sterratt, Murray{\_}2020{\_}Sequential neural likelihood Fast likelihood-free inference with autoregressive flows.pdf:pdf},
journal = {AISTATS 2019 - 22nd International Conference on Artificial Intelligence and Statistics},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
title = {{Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows}},
volume = {89},
year = {2020}
}
@article{Papamakarios2016,
abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an ∈-ball around the observed data, which is only correct in the limit ∈→0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as ∈ → 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
author = {Papamakarios, George and Murray, Iain},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Papamakarios, Murray{\_}2016{\_}Fast e-free inference of simulation models with Bayesian conditional density estimation.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
number = {Nips},
pages = {1036--1044},
title = {{Fast e-free inference of simulation models with Bayesian conditional density estimation}},
year = {2016}
}
@article{Cranmer2020,
abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.},
archivePrefix = {arXiv},
arxivId = {1911.01429},
author = {Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
doi = {10.1073/pnas.1912789117},
eprint = {1911.01429},
file = {:Users/lappalainenj/Documents/Mendeley Desktop/Cranmer, Brehmer, Louppe{\_}2020{\_}The frontier of simulation-based inference.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
mendeley-groups = {01 PhD Uni T{\"{u}}bingen/machine learning for scientific discovery,01 PhD Uni T{\"{u}}bingen/teaching/simulation-based inference},
pages = {201912789},
pmid = {32471948},
title = {{The frontier of simulation-based inference}},
year = {2020}
}
